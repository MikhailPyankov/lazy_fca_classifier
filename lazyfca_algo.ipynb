{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator, collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from itertools import product\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 23), (1000, 23))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train['target'] == 0].shape, train[train['target'] == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((286, 23), (214, 23))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test['target'] == 0].shape, test[test['target'] == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opp_check(item, to_check, opp_sample, min_cardinality):\n",
    "    dif = item - to_check\n",
    "    mask = dif == 0\n",
    "    if mask.sum() / mask.count() < min_cardinality:\n",
    "        return None\n",
    "    c_mask = opp_sample[opp_sample.columns[mask]]\n",
    "    c_mask = c_mask - item[mask]\n",
    "    aux = c_mask == 0\n",
    "    return aux.all(axis = 1).sum()\n",
    "\n",
    "def lazyfca(train, test, max_opposition, min_cardinality, return_report = False):\n",
    "    c_plus = train[train['target'] == 1]\n",
    "    c_minus = train[train['target'] == 0]\n",
    "    \n",
    "    def drop_target(df):\n",
    "        return df.drop(columns = ['target'])\n",
    "    \n",
    "    test_no_target = drop_target(test).copy()\n",
    "    test['pred'] = 0\n",
    "    c_plus = drop_target(c_plus)\n",
    "    c_minus = drop_target(c_minus)\n",
    "    count_plus = c_plus.count()[0]\n",
    "    count_minus = c_minus.count()[0]\n",
    "    i = 0\n",
    "    for i in range(len(test_no_target)):\n",
    "        item = test_no_target.iloc[i]\n",
    "        plus_votes_count = 0\n",
    "        minus_votes_count = 0\n",
    "        j = 0\n",
    "        for j in range(len(c_plus)):\n",
    "            plus_item = c_plus.iloc[j]\n",
    "            present_minus = opp_check(item, plus_item, c_minus, min_cardinality)\n",
    "            if present_minus is None:\n",
    "                continue\n",
    "            vote_minus = present_minus / count_minus\n",
    "            if vote_minus <= max_opposition:\n",
    "                plus_votes_count += 1\n",
    "        j = 0\n",
    "        for j in range(len(c_minus)):\n",
    "            minus_item = c_minus.iloc[j]\n",
    "            present_plus = opp_check(item, minus_item, c_plus, min_cardinality)\n",
    "            if present_plus is None:\n",
    "                continue\n",
    "            vote_plus = present_plus / count_plus\n",
    "            if vote_plus <= max_opposition:\n",
    "                minus_votes_count += 1\n",
    "        plus_votes_share = plus_votes_count / count_plus\n",
    "        minus_votes_share = minus_votes_count / count_minus\n",
    "        if plus_votes_share >= minus_votes_share:\n",
    "            test.iloc[i, test.columns.get_loc('pred')] = 1\n",
    "    if return_report == False:\n",
    "        return accuracy_score(test['target'], test['pred']), precision_score(test['target'], test['pred']), recall_score(test['target'], test['pred'])\n",
    "    return classification_report(test['target'], test['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation_acc(data, k, model_score, params):\n",
    "    data = data.sample(frac = 1, random_state = seed)\n",
    "    folds = np.array_split(data, k)\n",
    "    total_acc, total_pre, total_rec = 0, 0, 0\n",
    "    for i in range(len(folds)):\n",
    "        train = pd.concat(folds[:i] + folds[i+1:])\n",
    "        a, p, r = model_score(train, folds[i], **params)\n",
    "        total_acc += a\n",
    "        total_pre += p\n",
    "        total_rec += r\n",
    "    total_acc /= k\n",
    "    total_pre /= k\n",
    "    total_rec /= k\n",
    "    return total_acc, total_pre, total_rec\n",
    "\n",
    "def grid_search(model, param_grid, data, num_folds, print_all_results = False):\n",
    "    best_params, params = {}, {}\n",
    "    max_acc = 0\n",
    "    params_list = list(param_grid.keys())\n",
    "    values_list = [param_grid[v] for v in params_list]\n",
    "    params_combinations = list(product(*values_list))\n",
    "    for combination in params_combinations:\n",
    "        for param in params_list:\n",
    "            params[param] = combination[params_list.index(param)]\n",
    "        acc, pre, rec = cross_validation_acc(data, num_folds, model, params)\n",
    "        if acc > max_acc:\n",
    "            best_params = params.copy()\n",
    "            max_acc = acc\n",
    "        if print_all_results == True:\n",
    "            print(params, acc, pre, rec)\n",
    "    print('Best average accuracy during cross-validation:', max_acc)\n",
    "    print('Best hyperparameters:', best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_application(train, test, C, return_report = False):\n",
    "    \n",
    "    logit = LogisticRegression(C = C, random_state = seed)\n",
    "    logit.fit(train.drop(columns = ['target']), train['target'])\n",
    "    pred = logit.predict(test.drop(columns = ['target']))\n",
    "    \n",
    "    if return_report == False:\n",
    "        return accuracy_score(test['target'], test['pred']), precision_score(test['target'], test['pred']), recall_score(test['target'], test['pred'])\n",
    "    return classification_report(test['target'], pred)\n",
    "\n",
    "def rf_application(train, test, n_estimators, max_depth, return_report = False):\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, random_state = seed)\n",
    "    rf.fit(train.drop(columns = ['target']), train['target'])\n",
    "    pred = rf.predict(test.drop(columns = ['target']))\n",
    "    \n",
    "    if return_report == False:\n",
    "        return accuracy_score(test['target'], test['pred']), precision_score(test['target'], test['pred']), recall_score(test['target'], test['pred'])\n",
    "    return classification_report(test['target'], pred)\n",
    "\n",
    "def gbc_application(train, test, n_estimators, max_depth, return_report = False):\n",
    "    \n",
    "    rf = GradientBoostingClassifier(n_estimators = n_estimators, max_depth = max_depth, random_state = seed)\n",
    "    rf.fit(train.drop(columns = ['target']), train['target'])\n",
    "    pred = rf.predict(test.drop(columns = ['target']))\n",
    "    \n",
    "    if return_report == False:\n",
    "        return accuracy_score(test['target'], test['pred']), precision_score(test['target'], test['pred']), recall_score(test['target'], test['pred'])\n",
    "    return classification_report(test['target'], pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best average accuracy during cross-validation: 0.8869999999999999\n",
      "Best hyperparameters: {'C': 1}\n"
     ]
    }
   ],
   "source": [
    "grid_search(logit_application, {'C': [0.1, 1, 10]}, train, num_folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best average accuracy during cross-validation: 0.8885\n",
      "Best hyperparameters: {'n_estimators': 100, 'max_depth': 6}\n"
     ]
    }
   ],
   "source": [
    "grid_search(rf_application, {'n_estimators': [10, 50, 100], 'max_depth': [3, 6, None]}, train, num_folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best average accuracy during cross-validation: 0.889\n",
      "Best hyperparameters: {'n_estimators': 100, 'max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "grid_search(gbc_application, {'n_estimators': [10, 50, 100], 'max_depth': [3, 6, None]}, train, num_folds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_cardinality': 0.8, 'max_opposition': 0} 0.8755000000000001 0.8751587181223197 0.8769460976155077\n",
      "{'min_cardinality': 0.8, 'max_opposition': 0.01} 0.8779999999999999 0.8755166484707473 0.8820173473908849\n",
      "{'min_cardinality': 0.9, 'max_opposition': 0} 0.7264999999999999 0.665135979660483 0.9321700373816799\n",
      "{'min_cardinality': 0.9, 'max_opposition': 0.01} 0.7270000000000001 0.6666872657606395 0.929169291001623\n",
      "Best average accuracy during cross-validation: 0.8779999999999999\n",
      "Best hyperparameters: {'min_cardinality': 0.8, 'max_opposition': 0.01}\n"
     ]
    }
   ],
   "source": [
    "grid_search(lazyfca, {'min_cardinality': [0.8, 0.9], 'max_opposition': [0, 0.01]}, train, num_folds = 5, print_all_results = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.88      0.90       286\n",
      "           1       0.85      0.90      0.87       214\n",
      "\n",
      "    accuracy                           0.89       500\n",
      "   macro avg       0.88      0.89      0.88       500\n",
      "weighted avg       0.89      0.89      0.89       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(lazyfca(train, test, min_cardinality = 0.8, max_opposition = 0.01, accuracy_only = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.91      0.92       286\n",
      "           1       0.88      0.90      0.89       214\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.90      0.91      0.90       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(logit_application(train, test, C = 1, accuracy_only = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.91      0.91       286\n",
      "           1       0.88      0.89      0.89       214\n",
      "\n",
      "    accuracy                           0.90       500\n",
      "   macro avg       0.90      0.90      0.90       500\n",
      "weighted avg       0.90      0.90      0.90       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rf_application(train, test, n_estimators = 100, max_depth = 6, accuracy_only = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.92       286\n",
      "           1       0.87      0.91      0.89       214\n",
      "\n",
      "    accuracy                           0.91       500\n",
      "   macro avg       0.90      0.91      0.90       500\n",
      "weighted avg       0.91      0.91      0.91       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gbc_application(train, test, n_estimators = 100, max_depth = 3, accuracy_only = False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
